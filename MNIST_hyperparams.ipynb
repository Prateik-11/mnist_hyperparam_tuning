{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Classifier\n",
    "\n",
    "Uses the hyperparameters in the following cell to adjust the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 0.01\n",
    "HIDDEN_LAYERS = 10\n",
    "HIDDEN_LAYER_UNITS = 256\n",
    "DATA_SIZE = None # set to None, or if you want to nerf the model then set it to the number of training points to use\n",
    "DROPOUT = 0\n",
    "USE_BATCH_NORM = False\n",
    "L1 = 0\n",
    "L2 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on GPU: NVIDIA GeForce RTX 3060 Laptop GPU 😎\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Check if a GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    print('Running on GPU: ', end = \"\")\n",
    "    print(torch.cuda.get_device_name(0), end = \"\")\n",
    "    print(\" 😎\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('Running on CPU')\n",
    "    print(\"rip ☠️\")\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist_train = datasets.MNIST(root='.', train=True, transform=transforms.ToTensor(), download=True)\n",
    "mnist_train, mnist_val = torch.utils.data.random_split(mnist_train, [len(mnist_train) - 10000, 10000])\n",
    "if DATA_SIZE:\n",
    "    mnist_train, _ = torch.utils.data.random_split(mnist_train, [DATA_SIZE, len(mnist_train) - DATA_SIZE])\n",
    "\n",
    "mnist_test = datasets.MNIST(root='.', train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "# Create data loaders for the training and test sets\n",
    "train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=BATCH_SIZE, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(mnist_val, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Define the neural network\n",
    "class MNISTClassifier(nn.Module):\n",
    "    def __init__(self, hidden_layers, hidden_units, num_classes, dropout= DROPOUT, bn = USE_BATCH_NORM):\n",
    "        super().__init__()\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.hidden_units = hidden_units\n",
    "        self.layers = nn.ModuleList([nn.Linear(28 * 28, hidden_units)])\n",
    "        self.layers.extend([nn.Linear(hidden_units, hidden_units) for _ in range(hidden_layers - 1)])\n",
    "        self.layers.append(nn.Linear(hidden_units, num_classes))\n",
    "        self.dropout = dropout\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_units)\n",
    "        self.bn = bn\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, 28 * 28)\n",
    "        for i in range(self.hidden_layers):\n",
    "            x = torch.relu(self.layers[i](x))\n",
    "            if self.bn:\n",
    "                x = self.batch_norm(x)\n",
    "            x = F.dropout(x, self.dropout)\n",
    "        x = self.layers[-1](x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = MNISTClassifier(hidden_layers=HIDDEN_LAYERS, hidden_units=HIDDEN_LAYER_UNITS, num_classes=10).to(device)\n",
    "\n",
    "# Define the loss function and the optimizer\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Train the model\n",
    "def train(model, train_loader, val_loader, criterion, optimizer, epochs, L1, L2):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in tqdm(train_loader, total=len(train_loader), desc=f'Epoch {epoch + 1}/{epochs}'):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Get training statistics\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Add L1 regularization loss\n",
    "            reg_loss = 0\n",
    "            for param in model.parameters():\n",
    "                reg_loss += torch.sum(torch.abs(param))\n",
    "            loss += L1 * reg_loss\n",
    "\n",
    "            # Add L2 regularization loss\n",
    "            reg_loss = 0\n",
    "            for param in model.parameters():\n",
    "                reg_loss += torch.sum(torch.sum(param**2))\n",
    "            loss += 0.5 * L2 * reg_loss\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        accuracy = 100 * correct / total\n",
    "        val_accuracy, val_loss = validate(model, val_loader, criterion)\n",
    "        print(f'Training   | Loss: {running_loss / len(train_loader):.4f} | Accuracy: {accuracy:.2f}% ')\n",
    "        print(f'Validation | Loss: {val_loss / len(train_loader):.4f} | Accuracy: {val_accuracy:.2f}% ')\n",
    "\n",
    "\n",
    "# Test the model\n",
    "def validate(model, val_loader, criterion):\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        val_loss = 0\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        accuracy = 100 * correct / total\n",
    "        return accuracy, val_loss\n",
    "        # print()\n",
    "        # print(f'Accuracy of the network on the 10000 test images: {accuracy}')\n",
    "\n",
    "def test(model, test_loader):\n",
    "    print()\n",
    "    print(\"Results on test data:\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            all_labels.extend(labels.tolist())\n",
    "            all_preds.extend(predicted.tolist())\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f'Accuracy: {accuracy:.2f}%')\n",
    "        \n",
    "        # Calculate confusion matrix\n",
    "        confusion_matrix = torch.zeros(10, 10, dtype=torch.int64)\n",
    "        for actual, pred in zip(all_labels, all_preds):\n",
    "            confusion_matrix[actual, pred] += 1\n",
    "        print()\n",
    "        print(f'Confusion matrix: \\n{confusion_matrix}')\n",
    "        \n",
    "        # Calculate precision, recall and F1 score\n",
    "        precision = recall = f1 = 0\n",
    "        for i in range(10):\n",
    "            true_positive = confusion_matrix[i][i]\n",
    "            false_positive = confusion_matrix[:, i].sum() - true_positive\n",
    "            false_negative = confusion_matrix[i].sum() - true_positive\n",
    "            if true_positive > 0:\n",
    "                precision += true_positive / (true_positive + false_positive)\n",
    "                recall += true_positive / (true_positive + false_negative)\n",
    "                f1 += 2 * true_positive / (2 * true_positive + false_positive + false_negative)\n",
    "        precision /= 10\n",
    "        recall /= 10\n",
    "        f1 /= 10\n",
    "        print()\n",
    "        print(f'Precision: {precision:.2f}')\n",
    "        print(f'Recall: {recall:.2f}')\n",
    "        print(f'F1 Score: {f1:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3726e7160704fc78d9e6a4c6c1c6b30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training   | Loss: 2.2889 | Accuracy: 27.03% \n",
      "Validation | Loss: 0.4587 | Accuracy: 37.97% \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3b5d3861dc04c33b6169cf342a0f7b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/10:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training   | Loss: 2.2090 | Accuracy: 48.46% \n",
      "Validation | Loss: 0.4208 | Accuracy: 52.31% \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d23670eb9ff4aefa07b35c5c8afb499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/10:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training   | Loss: 1.6435 | Accuracy: 60.91% \n",
      "Validation | Loss: 0.2213 | Accuracy: 69.13% \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "786ccd69b54343e3bc3b075a03e238ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/10:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training   | Loss: 0.8469 | Accuracy: 75.44% \n",
      "Validation | Loss: 0.1427 | Accuracy: 79.09% \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7def7a4727ad4cd4a00ac8a207b23be4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/10:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training   | Loss: 0.6259 | Accuracy: 81.54% \n",
      "Validation | Loss: 0.1157 | Accuracy: 82.92% \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35b4b509ba7e479c8a64acb7c5044fb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/10:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training   | Loss: 0.5224 | Accuracy: 84.86% \n",
      "Validation | Loss: 0.1003 | Accuracy: 85.44% \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d437d3fc302419b9e67c38fbb7cdad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/10:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training   | Loss: 0.4590 | Accuracy: 86.75% \n",
      "Validation | Loss: 0.0899 | Accuracy: 87.17% \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c790b581fb64bc1909d8e6645b4e5de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/10:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training   | Loss: 0.4159 | Accuracy: 88.06% \n",
      "Validation | Loss: 0.0833 | Accuracy: 88.12% \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3fb562190154ba2b5d104deeda41b12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/10:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training   | Loss: 0.3841 | Accuracy: 89.04% \n",
      "Validation | Loss: 0.0784 | Accuracy: 88.70% \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c6101b429594886a29ae484119538e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/10:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training   | Loss: 0.3595 | Accuracy: 89.69% \n",
      "Validation | Loss: 0.0737 | Accuracy: 89.48% \n",
      "\n",
      "Results on test data:\n",
      "Accuracy: 89.99%\n",
      "\n",
      "Confusion matrix: \n",
      "tensor([[ 955,    0,    5,    1,    0,   10,    5,    2,    2,    0],\n",
      "        [   0, 1107,    1,    7,    0,    2,    3,    2,   13,    0],\n",
      "        [  15,   19,  898,   18,   10,    2,   19,   13,   34,    4],\n",
      "        [   3,    2,   24,  913,    0,   32,    0,   19,   13,    4],\n",
      "        [   1,    3,    5,    1,  888,    1,   15,    1,   10,   57],\n",
      "        [  17,    3,    8,   64,   10,  729,   13,    7,   33,    8],\n",
      "        [  21,    3,   10,    0,   14,   20,  885,    0,    5,    0],\n",
      "        [   4,   19,   23,    3,    4,    0,    0,  935,    3,   37],\n",
      "        [   5,   12,    8,   39,   10,   45,   17,    7,  808,   23],\n",
      "        [  12,    4,    1,   10,   39,   11,    0,   44,    7,  881]])\n",
      "\n",
      "Precision: 0.90\n",
      "Recall: 0.90\n",
      "F1 Score: 0.90\n"
     ]
    }
   ],
   "source": [
    "train(model, train_loader, val_loader, criterion, optimizer, NUM_EPOCHS, L1, L2)\n",
    "test(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f96c0a065a6f038b4d81e02ec5e62503dd3b71442db074018719e130d8db16de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
